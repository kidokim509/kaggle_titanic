{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import math, random, re\n",
    "from collections import defaultdict, Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from(weights): \n",
    "    \"\"\"i를 weights[i] / sum(weights)의 확률로 반환\"\"\"\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()    # 0과 total 사이를 균일하게 선택 (Return the next random floating point number in the range [0.0, 1.0).)\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w                     # 밑의 주석의 식을 만족하는 가장 작은 i를 반환\n",
    "        if rnd <= 0: return i        # weights[0] + ... + weights[i] >= rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 토픽이 각 문서에 할당되는 횟수\n",
    "# Counter로 구성된 list\n",
    "# 각각의 Counter는 각 문서를 의미함\n",
    "document_topic_counts = [Counter() for _ in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어가 각 토픽에 할당되는 횟수\n",
    "# Counter로 구성된 list\n",
    "topic_word_counts = [Counter() for _ in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 토픽에 할당되는 총 단어 수\n",
    "# 숫자로 구성된 list\n",
    "# 각각의 숫자는 각 토픽을 의미함\n",
    "topic_counts = [0 for _ in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문서에 포함되는 총 단어 수\n",
    "# 숫자로 구성된 list\n",
    "# 각각의 숫자는 각 문서를 의미함\n",
    "document_lengths = list(map(len, documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 종류 수\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "W = len(distinct_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topic_counts[3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_counts[2][\"nlp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    \"\"\"문서 d의 모든 단어 중에서 topic에 속하는\n",
    "    단어의 비율 (smoothing을 더한 비율)\"\"\"\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "           (document_lengths[d] + K * alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    \"\"\"topic에 속한 단어 중에서 word의 비율 (smoothing을 더한 비율)\"\"\"\n",
    "    return ((topic_word_counts[topic][word] + beta) / \n",
    "           (topic_counts[topic] + W * beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_weight(d, word, k):\n",
    "    \"\"\"문서와 문서의 단어가 주어지면,\n",
    "    k번째 토픽의 weight를 반환\"\"\"\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k)\n",
    "                        for k in range(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서의 각 단어가 임의의 토픽의 것이라고 가정한다.\n",
    "random.seed(0)\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                   for document in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 3, 0, 2, 3, 3, 2],\n",
       " [3, 2, 1, 1, 2],\n",
       " [1, 0, 2, 1, 2, 0],\n",
       " [0, 2, 3, 0, 2],\n",
       " [3, 2, 1, 3],\n",
       " [3, 2, 0, 0, 0, 3],\n",
       " [0, 3, 2, 1],\n",
       " [2, 0, 1, 1],\n",
       " [1, 1, 3, 0],\n",
       " [0, 2, 3, 0],\n",
       " [2, 2, 0],\n",
       " [2, 1, 2, 3],\n",
       " [0, 3, 2],\n",
       " [1, 2, 1, 1, 1],\n",
       " [0, 2, 3]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서의 각 단어가 임의의 토픽의 것이라고 가정한다. -> randrange로 할당\n",
    "document_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서내 토픽, 토픽내 단어 등장 카운트 하기\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1 # 문서의 토픽 매칭 카운트+1\n",
    "        topic_word_counts[topic][word] += 1  # 토픽의 단어 매칭 카운트+1\n",
    "        topic_counts[topic] += 1             # 토픽의 단어 개수+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 0, 0: 5, 2: 0, 1: 2})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document_topics에 나온 \"문서에 매칭되는 토픽 개수\"를 세어 놓음\n",
    "document_topic_counts[0]\n",
    "\n",
    "# [3, 3, 0, 2, 3, 3, 2] -> 3 4번, 0 1번 등..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Cassandra': 2,\n",
       "         'HBase': 3,\n",
       "         'Python': 0,\n",
       "         'numpy': 0,\n",
       "         'decision trees': 0,\n",
       "         'theory': 0,\n",
       "         'Mahout': 0,\n",
       "         'neural networks': 0,\n",
       "         'deep learning': 0,\n",
       "         'databases': 1,\n",
       "         'Postgres': 2,\n",
       "         'MySQL': 1,\n",
       "         'MongoDB': 2,\n",
       "         'probability': 0,\n",
       "         'scikit-learn': 0,\n",
       "         'NoSQL': 1,\n",
       "         'machine learning': 0,\n",
       "         'artificial intelligence': 0,\n",
       "         'Storm': 0,\n",
       "         'Spark': 0,\n",
       "         'pandas': 0,\n",
       "         'support vector machines': 0,\n",
       "         'Java': 0,\n",
       "         'statistics': 0,\n",
       "         'regression': 0,\n",
       "         'libsvm': 0,\n",
       "         'R': 0,\n",
       "         'programming languages': 0,\n",
       "         'Hadoop': 0,\n",
       "         'scipy': 0,\n",
       "         'Haskell': 0,\n",
       "         'Big Data': 0,\n",
       "         'C++': 0,\n",
       "         'MapReduce': 0,\n",
       "         'mathematics': 0,\n",
       "         'statsmodels': 0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_counts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 깁슨 샘플링. 계속 돌면 수렴한다.\n",
    "for iter in range(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "\n",
    "            # remove this word / topic from the counts\n",
    "            # so that it doesn't influence the weights\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "\n",
    "            # choose a new topic based on the weights\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "\n",
    "            # and now add it back to the counts\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Big Data 3\n",
      "0 Java 2\n",
      "0 artificial intelligence 2\n",
      "0 Hadoop 2\n",
      "0 deep learning 2\n",
      "0 pandas 1\n",
      "0 Spark 1\n",
      "0 Storm 1\n",
      "0 MapReduce 1\n",
      "0 neural networks 1\n",
      "1 HBase 3\n",
      "1 Cassandra 2\n",
      "1 Postgres 2\n",
      "1 MongoDB 2\n",
      "1 databases 1\n",
      "1 MySQL 1\n",
      "1 NoSQL 1\n",
      "2 Python 4\n",
      "2 R 4\n",
      "2 probability 3\n",
      "2 statistics 3\n",
      "2 statsmodels 2\n",
      "2 C++ 2\n",
      "2 Java 1\n",
      "2 scipy 1\n",
      "2 mathematics 1\n",
      "2 theory 1\n",
      "2 Haskell 1\n",
      "2 programming languages 1\n",
      "2 neural networks 1\n",
      "2 scikit-learn 1\n",
      "2 Mahout 1\n",
      "3 regression 3\n",
      "3 machine learning 2\n",
      "3 libsvm 2\n",
      "3 support vector machines 1\n",
      "3 decision trees 1\n",
      "3 pandas 1\n",
      "3 scikit-learn 1\n",
      "3 numpy 1\n",
      "['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra']\n",
      "Counter({0: 5, 1: 2, 3: 0, 2: 0})\n",
      "Big Data and programming languages 5\n",
      "Python and statistics 2\n",
      "\n",
      "['NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'Postgres']\n",
      "Counter({1: 5, 3: 0, 2: 0, 0: 0})\n",
      "Python and statistics 5\n",
      "\n",
      "['Python', 'scikit-learn', 'scipy', 'numpy', 'statsmodels', 'pandas']\n",
      "Counter({2: 3, 3: 3, 1: 0, 0: 0})\n",
      "databases 3\n",
      "machine learning 3\n",
      "\n",
      "['R', 'Python', 'statistics', 'regression', 'probability']\n",
      "Counter({2: 4, 3: 1, 0: 0, 1: 0})\n",
      "databases 4\n",
      "machine learning 1\n",
      "\n",
      "['machine learning', 'regression', 'decision trees', 'libsvm']\n",
      "Counter({3: 4, 2: 0, 1: 0, 0: 0})\n",
      "machine learning 4\n",
      "\n",
      "['Python', 'R', 'Java', 'C++', 'Haskell', 'programming languages']\n",
      "Counter({2: 6, 3: 0, 0: 0, 1: 0})\n",
      "databases 6\n",
      "\n",
      "['statistics', 'probability', 'mathematics', 'theory']\n",
      "Counter({2: 4, 0: 0, 3: 0, 1: 0})\n",
      "databases 4\n",
      "\n",
      "['machine learning', 'scikit-learn', 'Mahout', 'neural networks']\n",
      "Counter({2: 3, 3: 1, 0: 0, 1: 0})\n",
      "databases 3\n",
      "machine learning 1\n",
      "\n",
      "['neural networks', 'deep learning', 'Big Data', 'artificial intelligence']\n",
      "Counter({0: 4, 1: 0, 3: 0, 2: 0})\n",
      "Big Data and programming languages 4\n",
      "\n",
      "['Hadoop', 'Java', 'MapReduce', 'Big Data']\n",
      "Counter({0: 4, 2: 0, 3: 0, 1: 0})\n",
      "Big Data and programming languages 4\n",
      "\n",
      "['statistics', 'R', 'statsmodels']\n",
      "Counter({2: 3, 0: 0, 3: 0, 1: 0})\n",
      "databases 3\n",
      "\n",
      "['C++', 'deep learning', 'artificial intelligence', 'probability']\n",
      "Counter({2: 2, 0: 2, 1: 0, 3: 0})\n",
      "databases 2\n",
      "Big Data and programming languages 2\n",
      "\n",
      "['pandas', 'R', 'Python']\n",
      "Counter({2: 2, 0: 1, 3: 0, 1: 0})\n",
      "databases 2\n",
      "Big Data and programming languages 1\n",
      "\n",
      "['databases', 'HBase', 'Postgres', 'MySQL', 'MongoDB']\n",
      "Counter({1: 5, 2: 0, 3: 0, 0: 0})\n",
      "Python and statistics 5\n",
      "\n",
      "['libsvm', 'regression', 'support vector machines']\n",
      "Counter({3: 3, 0: 0, 2: 0, 1: 0})\n",
      "machine learning 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, word_counts in enumerate(topic_word_counts):\n",
    "    for word, count in word_counts.most_common():\n",
    "        if count > 0: print(k, word, count)\n",
    "\n",
    "topic_names = [\"Big Data and programming languages\",\n",
    "               \"Python and statistics\",\n",
    "               \"databases\",\n",
    "               \"machine learning\"]\n",
    "i=0\n",
    "for document, topic_counts in zip(documents, document_topic_counts):\n",
    "    print(document)\n",
    "    print(document_topic_counts[i])\n",
    "    i = i+1\n",
    "    for topic, count in topic_counts.most_common():\n",
    "        if count > 0:\n",
    "            print(topic_names[topic], count)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1, 0, 0, 0, 1],\n",
       " [1, 1, 1, 1, 1],\n",
       " [2, 3, 2, 3, 2, 3],\n",
       " [2, 2, 2, 3, 2],\n",
       " [3, 3, 3, 3],\n",
       " [2, 2, 2, 2, 2, 2],\n",
       " [2, 2, 2, 2],\n",
       " [3, 2, 2, 2],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [2, 2, 2],\n",
       " [2, 0, 0, 2],\n",
       " [0, 2, 2],\n",
       " [1, 1, 1, 1, 1],\n",
       " [3, 3, 3]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
